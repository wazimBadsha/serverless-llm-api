```markdown
# Serverless LLM API with Node.js and AWS Lambda

A production-ready template for deploying Large Language Models (LLMs) as serverless APIs using AWS Lambda and Docker. Ideal for scalable, cost-efficient AI inference.

[![Medium Article](https://img.shields.io/badge/ğŸ“–_Read_Medium_Article-gray?logo=medium)](https://medium.com/@muhammadbadhusha/building-scalable-llm-powered-apis-with-node-js-and-aws-lambda-a-serverless-guide-1057a6b0fb0d)

## Features
- ğŸ³ **Dockerized** pipeline for models up to 10GB
- âš¡ **Cold-start optimization** with model pre-loading
- ğŸ”’ **Input validation** and error handling
- ğŸ“¦ **Serverless Framework** deployment

## ğŸ‘¥ Who Is This For?
- **ML Engineers**: Simplify LLM deployment without Kubernetes/EC2
- **DevOps Teams**: Scalable AI endpoints with serverless best practices
- **Startups**: Reduce cloud costs while maintaining performance

## ğŸ› ï¸ Project Structure
```
serverless-llm-api/
â”œâ”€â”€ Dockerfile            # Docker config with model caching
â”œâ”€â”€ lambda.js             # Lambda function with Hugging Face pipeline
â”œâ”€â”€ serverless.yml        # AWS Lambda deployment config
â”œâ”€â”€ package.json          # Node.js dependencies
â””â”€â”€ README.md             # You are here!
```

## ğŸš€ Setup

### Prerequisites
- Node.js 18+
- AWS CLI configured
- Docker installed
- Serverless Framework (`npm install -g serverless`)

1. **Clone the repo**:
   ```bash
   git clone https://github.com/your-username/serverless-llm-api.git
   cd serverless-llm-api
   ```

2. **Install dependencies**:
   ```bash
   npm install
   ```

3. **Deploy to AWS**:
   ```bash
   npm run deploy
   ```

## ğŸŒŸ Usage
```bash
curl -X POST https://YOUR_API_URL/classify \
  -H "Content-Type: application/json" \
  -d '{"text": "Serverless machine learning is revolutionary!"}'
```

**Example Response**:
```json
[{
  "label": "POSITIVE",
  "score": 0.998
}]
```

## âš™ï¸ Configuration
Key settings in `serverless.yml`:
```yaml
memorySize: 10240  # 10GB (AWS Lambda max)
timeout: 900       # 15-minute timeout
```

## ğŸ”„ Customization
To use different models (e.g., GPT-2, Llama 2):
1. Update `Dockerfile`:
   ```dockerfile
   RUN npx transformers-cli download MODEL_NAME --format onnx
   ```
2. Modify `lambda.js`:
   ```javascript
   const classifier = await pipeline('text-classification', 'MODEL_NAME');
   ```

## ğŸ¤ Contributing
PRs welcome! For major changes, open an issue first.

## License
MIT
```
